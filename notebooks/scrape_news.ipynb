{"cells":[{"cell_type":"code","execution_count":null,"id":"974a7dc9","metadata":{"id":"974a7dc9"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import requests\n","import pandas as pd\n","import time\n","from datetime import datetime\n","from dateutil import parser\n","import re\n","from collections import Counter\n","import os"]},{"cell_type":"code","source":["class BitcoinTalkScraper:\n","\n","  def __init__(self, url):\n","    # Initialize the scraper with the URL to scrape from\n","    self.url = url\n","    # Dictionary to store scraped data (date, headline, link, description)\n","    self.news_dict = {\"date\": [],\n","                      \"headline\": [],\n","                      \"headline_link\": [],\n","                      \"description\": []\n","              }\n","    # Placeholder attributes for headlines, descriptions, and dates\n","    self.headline = \"\"\n","    self.description = \"\"\n","    self.link = \"\"\n","    self.date = None\n","\n","\n","  def parse_date(self,date_str):\n","    \"\"\"\n","    Attempt to parse the date string in different formats using dateutil.\n","    First try without assuming the day-first format, then retry with day-first.\n","    Return the date in YYYY-MM-DD format, or None if parsing fails.\n","    \"\"\"\n","    try:\n","        # Use dateutil parser to parse the date\n","        date = parser.parse(date_str, dayfirst=False, yearfirst=True)\n","        return date.strftime(\"%Y-%m-%d\")\n","    except ValueError:\n","        try:\n","            # Try parsing with day first if the previous attempt failed\n","            date = parser.parse(date_str, dayfirst=True, yearfirst=False)\n","            return date.strftime(\"%Y-%m-%d\")\n","        except ValueError:\n","            return None\n","\n","  def extract_date(self):\n","    \"\"\"\n","    Extract a date from the headline using regular expressions.\n","    The method searches for date patterns and separates the date from the headline text.\n","    If no date is found, it marks the date as 'Null' or leaves the headline unchanged.\n","    \"\"\"\n","    #regex pattern for extracting different date formats from the headline\n","    date_pattern = re.compile(r'((\\[(\\d{4}\\-\\d{1,2}\\-\\d{1,2)}\\])|(\\d{4}\\-\\d{1,2}\\-\\d{1,2})|(\\d{4}\\/\\d{1,2}\\/\\d{1,2})|(\\d{1,2}\\-\\d{1,2}\\-\\d{4})|(\\d{1,2}\\/\\d{1,2}\\/\\d{4})|(\\d{4}.\\d{1,2}\\.\\d{1,2})|(\\d{1,2}.\\d{1,2}\\.\\d{4}))')\n","    #regex pattern for handling headlines without date (only text)\n","    text_without_date = re.compile(r'^[a-zA-Z]+')\n","    #checking if date is present with date_pattern\n","    date_match = date_pattern.search(self.headline)\n","    #checking for text with only alphabets\n","    alpha_match = text_without_date.search(self.headline)\n","    if date_match: #If date_pattern is matched, extracting and splitting date and headline\n","        date = self.parse_date(date_match.group()) #Extract date\n","        text = self.headline[date_match.end()+1:] #Extarct rest of the string as headline text\n","        self.headline = text\n","        self.date = date\n","        # date_headline.append([date,text]) #Appending data as a list of date and headline text\n","\n","\n","\n","\n","    elif alpha_match: #If no date_pattern is matched, date is set as 'Null' and appended to date_headline list\n","        self.date = \"Null\"\n","        # text = self.headline\n","        # date_headline.append([date,text])\n","    else:\n","        self.date = self.headline\n","        # count+=1\n","\n","\n","  def scrape_data(self):\n","    \"\"\"\n","    Scrapes headlines, dates, and descriptions from the given URL.\n","    Extracts data from the page, processes it, and stores it in a DataFrame.\n","    \"\"\"\n","    # Make an get() request to the specified URL\n","    headlines_res = requests.get(url=self.url)\n","    time.sleep(1) # Wait to avoid overwhelming the server\n","\n","    # Parse the HTML content of the response using BeautifulSoup\n","    headlines_soup = BeautifulSoup(headlines_res.content,'html.parser')\n","    #Finding the area containing headlines\n","    bodyarea = headlines_soup.find('div',id='bodyarea')\n","    headlines_table = (bodyarea.find('div', class_ = 'tborder')).find('table',class_='bordercolor')\n","    headlines = headlines_table.find_all('td',class_='windowbg')\n","\n","    # Loop through each headline and scrape the relevant data\n","    for headline in headlines:\n","        if headline.find('span'):\n","            headline_text = headline.find('span').get_text()  # Get the text of the headline\n","            headline_link = headline.find('a').get('href')  # Get the href attribute (link) of the headline\n","            try:\n","              description_res = requests.get(headline_link) #Make an get() request to the specified link\n","              time.sleep(1) # Wait to avoid overwhelming the server\n","              description_soup = BeautifulSoup(description_res.content,'html.parser') # Parse the HTML content of the description response using BeautifulSoup\n","              #Finding the area containing description\n","              description = description_soup.find('div',class_='post').get_text()\n","\n","            except Exception as e:\n","              # Handle any errors during the request\n","              description = \"Failed to retrieve description\"\n","            # Store the scraped headline, description, and link\n","            self.headline = headline_text\n","            self.link = headline_link\n","            self.description = description\n","\n","            # Extract the date from the headline\n","            self.extract_date()\n","\n","            # Append the data to the dictionary\n","            self.news_dict['date'].append(self.date)\n","            self.news_dict['headline'].append(self.headline)\n","            self.news_dict['headline_link'].append(self.link)\n","            self.news_dict['description'].append(self.description)\n","\n","            # Reset variables for the next headline\n","            self.headline = \"\"\n","            self.description = \"\"\n","            self.link = \"\"\n","            self.date = None\n","\n","    # Return the collected data as a DataFrame\n","    return pd.DataFrame(self.news_dict)\n","\n","\n"],"metadata":{"id":"KnP0w6SEz9YR"},"id":"KnP0w6SEz9YR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_to_csv(df, file_name):\n","    \"\"\"\n","    Save data to a CSV file. If the file exists, it appends data.\n","    If the file does not exist, it creates it and writes the header.\n","    \"\"\"\n","    # Check if the file already exists\n","    file_exists = os.path.isfile(file_name)\n","\n","    # Append data if file exists, otherwise create a new file\n","    df.to_csv(file_name, mode='a', index=False, header=not file_exists)\n","\n"],"metadata":{"id":"xZM-HnuYxO37"},"id":"xZM-HnuYxO37","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"c635e799","metadata":{"id":"c635e799"},"outputs":[],"source":["# Initialize URL variables\n","current_url = \"https://bitcointalk.org/index.php?board=77.0\" # News headlines BitcoinTalk website base_url\n","next_url = \"\"\n","last_page = False\n","news_file_path = \"data/unlabeled_news.csv\"\n","\n","while not last_page:\n","  # Scrape current page data\n","  current_page_scraper = BitcoinTalkScraper(current_url)\n","  current_page_data = current_page_scraper.scrape_data()\n","\n","  # Save the scraped data to a CSV file\n","  save_to_csv(current_page_data, news_file_path)\n","\n","  # Fetch and parse the current page to find the next page link\n","  homepage_response = requests.get(current_url) # Fetch the content of the first page\n","  time.sleep(1)\n","  homepage_soup = BeautifulSoup(homepage_response.content,'html.parser') # Parse the HTML content using BeautifulSoup\n","\n","  # Locate the previous or next page number navigation links\n","  prevnext_links = (homepage_soup.find('div',id='bodyarea')).find('td',id='toppages').find_all('span',class_ = 'prevnext')\n","\n","  # Determine if it's the last page (if only one link exists and it's the \"Previous\" link)\n","  if len(prevnext_links)==1 and next_url !=\"\":\n","    last_page = True\n","\n","  # Find the next page URL\n","  next_link_tag = prevnext_links[-1].find('a', class_='navPages')  # Find the \"Next\" link\n","  if next_link_tag:\n","      next_url = next_link_tag.get('href')\n","      current_url = next_url  # Set current_url to the next page's URL\n","      # print(f\"Next URL: {next_url}\")\n","  else:\n","      last_page = True  # No \"Next\" link means we've reached the last page\n","\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}